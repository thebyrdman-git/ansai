---
# Universal Self-Healing System Configuration
# Applies to ALL services on testserver

# Owner email for all healing reports
owner_email: your-email@example.com

# SMTP configuration for email alerts
smtp_server: smtp.gmail.com
smtp_port: 587
smtp_user: your-email@example.com
smtp_password: "{{ lookup('env', 'ANSAI_SMTP_PASSWORD') }}"

# Services to monitor and self-heal
# Add any service here to enable automatic healing
monitored_services:
  - name: my-flask-app
    port: 5000
    domain: app.example.com
    critical: true
    healing_strategies:
      - service_restart
      - database_check
      - port_conflict
      - environment_check
    
  - name: my-api
    port: 5001
    domain: api.example.com
    critical: true
    healing_strategies:
      - service_restart
      - database_check
      - port_conflict
      - environment_check
  
  - name: traefik
    port: 80
    domain: null
    critical: true
    healing_strategies:
      - service_restart
      - config_validation
      - port_conflict
  
  # Socket-activated service example (like cockpit)
  # - name: cockpit
  #   port: 9090
  #   socket_activated: true    # Use .socket instead of .service
  #   critical: false
  #   healing_strategies:
  #     - service_restart
  
  # Add more services as needed
  # - name: service-name
  #   port: port-number
  #   domain: domain.example.com
  #   critical: true/false
  #   socket_activated: false   # Set to true for socket-activated services
  #   healing_strategies:
  #     - service_restart
  #     - custom_strategy

# Healing check interval (minutes)
check_interval: 2

# Failure threshold before healing attempt
failure_threshold: 3

# Self-healing enabled/disabled
self_healing_enabled: true

# Alert on every healing attempt (true) or only failures (false)
alert_on_success: true

# ============================================================================
# AI-Powered Root Cause Analysis
# ============================================================================
ai_analysis_enabled: true

# AI Backend: groq, ollama, litellm, openai
# - groq: Fast cloud inference (free tier available)
# - ollama: 100% local, no API keys needed
# - litellm: Multi-model proxy (use ansai-litellm-proxy)
# - openai: OpenAI API directly
ai_backend: groq

# Groq Configuration (default - fast & free tier)
groq_api_key: "{{ lookup('env', 'ANSAI_GROQ_API_KEY') }}"
groq_model: llama-3.1-8b-instant
groq_api_url: https://api.groq.com/openai/v1/chat/completions

# Ollama Configuration (local - no API keys needed)
ollama_url: http://localhost:11434
ollama_model: llama3

# LiteLLM Configuration (proxy - any model)
litellm_url: http://localhost:4000
litellm_model: gpt-4

# OpenAI Configuration (direct)
openai_api_key: "{{ lookup('env', 'OPENAI_API_KEY') }}"
openai_model: gpt-4o-mini
openai_api_url: https://api.openai.com/v1/chat/completions

# ============================================================================
# Alert Configuration  
# ============================================================================
# Alert method: email, webhook, both, none
alert_method: email

# Webhook Configuration (Slack, Discord, etc.)
webhook_url: ""
webhook_format: slack  # slack, discord, generic

