#!/bin/bash
# ansai-ask - Interactive AI assistant for infrastructure questions
# Part of ANSAI - AI-Powered Automation Infrastructure
# https://ansai.dev

set -euo pipefail

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'
BOLD='\033[1m'

# Configuration
AI_BACKEND="${ANSAI_AI_BACKEND:-auto}"
GROQ_API_KEY="${ANSAI_GROQ_API_KEY:-${GROQ_API_KEY:-}}"
OLLAMA_URL="${ANSAI_OLLAMA_URL:-http://localhost:11434}"
OLLAMA_MODEL="${ANSAI_OLLAMA_MODEL:-llama3}"
OPENAI_API_KEY="${OPENAI_API_KEY:-}"
LITELLM_URL="${ANSAI_LITELLM_URL:-http://localhost:4000}"

show_help() {
    cat << 'EOF'
ansai-ask - Ask AI questions about your infrastructure

USAGE:
    ansai-ask "your question here"
    ansai-ask --analyze /var/log/syslog
    echo "log data" | ansai-ask --stdin "what's wrong?"
    ansai-ask --interactive

OPTIONS:
    -h, --help          Show this help message
    -a, --analyze FILE  Analyze a log file
    -s, --stdin         Read additional context from stdin
    -i, --interactive   Interactive chat mode
    -b, --backend NAME  AI backend: auto, groq, ollama, openai, litellm
    -m, --model MODEL   Model to use (overrides default)
    --service NAME      Include service status in context
    --last DURATION     Include logs from last duration (e.g., "1h", "30m")

BACKENDS:
    auto    - Auto-detect available backend (ollama → groq → openai → litellm)
    groq    - Groq API (fast, free tier) - requires ANSAI_GROQ_API_KEY
    ollama  - Local Ollama (free, private) - requires ollama running
    openai  - OpenAI API - requires OPENAI_API_KEY
    litellm - LiteLLM proxy - requires proxy running

ENVIRONMENT VARIABLES:
    ANSAI_AI_BACKEND    Default backend (auto, groq, ollama, openai, litellm)
    ANSAI_GROQ_API_KEY  Groq API key
    ANSAI_OLLAMA_URL    Ollama URL (default: http://localhost:11434)
    ANSAI_OLLAMA_MODEL  Ollama model (default: llama3)
    OPENAI_API_KEY      OpenAI API key
    ANSAI_LITELLM_URL   LiteLLM proxy URL (default: http://localhost:4000)

EXAMPLES:
    # Simple question
    ansai-ask "why might nginx fail to start?"

    # Analyze a log file
    ansai-ask --analyze /var/log/nginx/error.log

    # Ask about a specific service
    ansai-ask --service nginx "why is it using so much memory?"

    # Pipe logs for analysis
    journalctl -u myapp --since "1 hour ago" | ansai-ask --stdin "what's causing these errors?"

    # Interactive mode
    ansai-ask --interactive

    # Use specific backend
    ansai-ask --backend ollama "explain this error"

EOF
}

print_info() {
    echo -e "${CYAN}ℹ️  $1${NC}" >&2
}

print_error() {
    echo -e "${RED}❌ $1${NC}" >&2
}

print_success() {
    echo -e "${GREEN}✅ $1${NC}" >&2
}

# Detect available AI backend
detect_backend() {
    # Try Ollama first (local, free)
    if curl -s "$OLLAMA_URL/api/tags" >/dev/null 2>&1; then
        echo "ollama"
        return
    fi
    
    # Try Groq (fast, free tier)
    if [ -n "$GROQ_API_KEY" ]; then
        echo "groq"
        return
    fi
    
    # Try OpenAI
    if [ -n "$OPENAI_API_KEY" ]; then
        echo "openai"
        return
    fi
    
    # Try LiteLLM proxy
    if curl -s "$LITELLM_URL/health" >/dev/null 2>&1; then
        echo "litellm"
        return
    fi
    
    echo "none"
}

# Call AI with the given prompt
call_ai() {
    local prompt="$1"
    local backend="$2"
    local model="${3:-}"
    
    case "$backend" in
        groq)
            model="${model:-llama-3.1-8b-instant}"
            curl -s -X POST "https://api.groq.com/openai/v1/chat/completions" \
                -H "Authorization: Bearer $GROQ_API_KEY" \
                -H "Content-Type: application/json" \
                -d @- <<EOFCURL | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('choices', [{}])[0].get('message', {}).get('content', 'No response'))"
{
    "model": "$model",
    "messages": [
        {"role": "system", "content": "You are an expert Linux/DevOps engineer. Provide clear, actionable answers. Be concise but thorough."},
        {"role": "user", "content": $(echo "$prompt" | python3 -c "import sys, json; print(json.dumps(sys.stdin.read()))")}
    ],
    "temperature": 0.3,
    "max_tokens": 1000
}
EOFCURL
            ;;
        ollama)
            model="${model:-$OLLAMA_MODEL}"
            curl -s -X POST "$OLLAMA_URL/api/chat" \
                -H "Content-Type: application/json" \
                -d @- <<EOFCURL | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('message', {}).get('content', 'No response'))"
{
    "model": "$model",
    "messages": [
        {"role": "system", "content": "You are an expert Linux/DevOps engineer. Provide clear, actionable answers. Be concise but thorough."},
        {"role": "user", "content": $(echo "$prompt" | python3 -c "import sys, json; print(json.dumps(sys.stdin.read()))")}
    ],
    "stream": false
}
EOFCURL
            ;;
        openai)
            model="${model:-gpt-4o-mini}"
            curl -s -X POST "https://api.openai.com/v1/chat/completions" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d @- <<EOFCURL | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('choices', [{}])[0].get('message', {}).get('content', 'No response'))"
{
    "model": "$model",
    "messages": [
        {"role": "system", "content": "You are an expert Linux/DevOps engineer. Provide clear, actionable answers. Be concise but thorough."},
        {"role": "user", "content": $(echo "$prompt" | python3 -c "import sys, json; print(json.dumps(sys.stdin.read()))")}
    ],
    "temperature": 0.3,
    "max_tokens": 1000
}
EOFCURL
            ;;
        litellm)
            model="${model:-gpt-4}"
            curl -s -X POST "$LITELLM_URL/v1/chat/completions" \
                -H "Content-Type: application/json" \
                -d @- <<EOFCURL | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('choices', [{}])[0].get('message', {}).get('content', 'No response'))"
{
    "model": "$model",
    "messages": [
        {"role": "system", "content": "You are an expert Linux/DevOps engineer. Provide clear, actionable answers. Be concise but thorough."},
        {"role": "user", "content": $(echo "$prompt" | python3 -c "import sys, json; print(json.dumps(sys.stdin.read()))")}
    ],
    "temperature": 0.3,
    "max_tokens": 1000
}
EOFCURL
            ;;
    esac
}

# Interactive mode
interactive_mode() {
    local backend="$1"
    local model="$2"
    
    echo -e "${BLUE}${BOLD}╔═══════════════════════════════════════════════════╗${NC}"
    echo -e "${BLUE}${BOLD}║         ANSAI Interactive AI Assistant            ║${NC}"
    echo -e "${BLUE}${BOLD}╚═══════════════════════════════════════════════════╝${NC}"
    echo ""
    echo -e "${CYAN}Backend: $backend${NC}"
    echo -e "${CYAN}Type 'exit' or 'quit' to leave, 'clear' to reset${NC}"
    echo ""
    
    local context=""
    
    while true; do
        echo -en "${GREEN}You: ${NC}"
        read -r question
        
        case "$question" in
            exit|quit|q)
                echo -e "${YELLOW}Goodbye!${NC}"
                exit 0
                ;;
            clear)
                context=""
                echo -e "${YELLOW}Context cleared.${NC}"
                continue
                ;;
            "")
                continue
                ;;
        esac
        
        echo ""
        echo -en "${BLUE}AI: ${NC}"
        
        local full_prompt="$question"
        if [ -n "$context" ]; then
            full_prompt="Context from previous conversation:\n$context\n\nNew question: $question"
        fi
        
        local response
        response=$(call_ai "$full_prompt" "$backend" "$model")
        echo "$response"
        echo ""
        
        # Add to context for follow-up questions
        context="${context}\nQ: $question\nA: $response"
    done
}

# Main
main() {
    local question=""
    local analyze_file=""
    local read_stdin=false
    local interactive=false
    local backend="$AI_BACKEND"
    local model=""
    local service=""
    local last_duration=""
    local stdin_content=""
    
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h|--help)
                show_help
                exit 0
                ;;
            -a|--analyze)
                analyze_file="$2"
                shift 2
                ;;
            -s|--stdin)
                read_stdin=true
                shift
                ;;
            -i|--interactive)
                interactive=true
                shift
                ;;
            -b|--backend)
                backend="$2"
                shift 2
                ;;
            -m|--model)
                model="$2"
                shift 2
                ;;
            --service)
                service="$2"
                shift 2
                ;;
            --last)
                last_duration="$2"
                shift 2
                ;;
            *)
                question="$1"
                shift
                ;;
        esac
    done
    
    # Read from stdin if requested
    if [ "$read_stdin" = true ]; then
        stdin_content=$(cat)
    fi
    
    # Auto-detect backend if needed
    if [ "$backend" = "auto" ]; then
        backend=$(detect_backend)
        if [ "$backend" = "none" ]; then
            print_error "No AI backend available!"
            echo ""
            echo "Options:"
            echo "  1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh"
            echo "  2. Set ANSAI_GROQ_API_KEY (free: https://console.groq.com)"
            echo "  3. Set OPENAI_API_KEY"
            echo "  4. Start ansai-litellm-proxy"
            exit 1
        fi
        print_info "Using backend: $backend"
    fi
    
    # Interactive mode
    if [ "$interactive" = true ]; then
        interactive_mode "$backend" "$model"
        exit 0
    fi
    
    # Build context
    local context=""
    
    # Add file content if analyzing
    if [ -n "$analyze_file" ]; then
        if [ ! -f "$analyze_file" ]; then
            print_error "File not found: $analyze_file"
            exit 1
        fi
        context="FILE CONTENT ($analyze_file):\n$(tail -100 "$analyze_file")\n\n"
        question="${question:-Analyze this log file and identify any issues or errors.}"
    fi
    
    # Add stdin content
    if [ -n "$stdin_content" ]; then
        context="${context}INPUT DATA:\n$stdin_content\n\n"
    fi
    
    # Add service status if requested
    if [ -n "$service" ]; then
        local status=$(systemctl status "$service" --no-pager 2>&1 | head -30)
        local logs=""
        if [ -n "$last_duration" ]; then
            logs=$(journalctl -u "$service" --since "$last_duration ago" --no-pager 2>&1 | tail -50)
        else
            logs=$(journalctl -u "$service" -n 30 --no-pager 2>&1)
        fi
        context="${context}SERVICE STATUS ($service):\n$status\n\nRECENT LOGS:\n$logs\n\n"
    fi
    
    # Check we have a question
    if [ -z "$question" ]; then
        print_error "No question provided!"
        echo ""
        echo "Usage: ansai-ask \"your question\""
        echo "       ansai-ask --help"
        exit 1
    fi
    
    # Build full prompt
    local full_prompt=""
    if [ -n "$context" ]; then
        full_prompt="$context\nQUESTION: $question"
    else
        full_prompt="$question"
    fi
    
    # Call AI
    echo ""
    call_ai "$full_prompt" "$backend" "$model"
    echo ""
}

main "$@"

