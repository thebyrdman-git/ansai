#!/bin/bash
# ansai-litellm-proxy - Multi-model LLM proxy with intelligent routing
# Part of ANSAI AI Integration Building Blocks
# https://ansai.dev

set -euo pipefail

# Configuration
CONFIG_FILE="${LITELLM_CONFIG:-$HOME/.config/litellm/config.yaml}"
DEFAULT_PORT="4000"
DEFAULT_HOST="0.0.0.0"

# Parse command line arguments
show_help() {
    cat << EOF
ansai-litellm-proxy - Start LiteLLM proxy server for multi-model LLM access

USAGE:
    ansai-litellm-proxy [port] [host]

ARGUMENTS:
    port    Port to listen on (default: 4000)
    host    Host to bind to (default: 0.0.0.0)

ENVIRONMENT VARIABLES:
    LITELLM_CONFIG     Path to configuration file (default: ~/.config/litellm/config.yaml)
    OPENAI_API_KEY     API key for OpenAI models
    ANTHROPIC_API_KEY  API key for Anthropic (Claude) models
    GROQ_API_KEY       API key for Groq models

EXAMPLES:
    # Start with defaults
    ansai-litellm-proxy

    # Custom port
    ansai-litellm-proxy 8080

    # Custom host and port
    ansai-litellm-proxy 8080 127.0.0.1

    # With custom config
    LITELLM_CONFIG=./my-config.yaml ansai-litellm-proxy

FEATURES:
    - Multi-model support (OpenAI, Claude, Groq, local models)
    - OpenAI-compatible API endpoints
    - Automatic fallback between models
    - Cost tracking and budget limits
    - Request caching
    - Load balancing

CONFIGURATION:
    Create ~/.config/litellm/config.yaml with your model configuration.
    See: https://docs.litellm.ai/docs/proxy/configs

    Example config:
        model_list:
          - model_name: gpt-4
            litellm_params:
              model: gpt-4
              api_key: your-openai-key
          - model_name: claude
            litellm_params:
              model: claude-3-opus
              api_key: your-anthropic-key

EXIT CODES:
    0   Success
    1   Configuration or dependency error

EOF
}

# Parse arguments
if [[ "${1:-}" == "--help" ]] || [[ "${1:-}" == "-h" ]]; then
    show_help
    exit 0
fi

PORT="${1:-$DEFAULT_PORT}"
HOST="${2:-$DEFAULT_HOST}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Output functions
print_status() {
    echo -e "${BLUE}[ANSAI-LiteLLM]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[ANSAI-LiteLLM]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[ANSAI-LiteLLM]${NC} $1"
}

print_error() {
    echo -e "${RED}[ANSAI-LiteLLM]${NC} $1"
}

# Check if litellm is installed
if ! command -v litellm &> /dev/null; then
    print_error "LiteLLM is not installed"
    print_status "Install with: pip install 'litellm[proxy]'"
    print_status "Documentation: https://docs.litellm.ai/"
    exit 1
fi

# Check if config file exists
if [[ ! -f "$CONFIG_FILE" ]]; then
    print_warning "Configuration file not found: $CONFIG_FILE"
    print_status ""
    print_status "Creating example configuration..."
    
    mkdir -p "$(dirname "$CONFIG_FILE")"
    
    cat > "$CONFIG_FILE" << 'EOFCONFIG'
# LiteLLM Proxy Configuration
# https://docs.litellm.ai/docs/proxy/configs

model_list:
  # OpenAI Models (requires OPENAI_API_KEY)
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
  
  # Anthropic Claude (requires ANTHROPIC_API_KEY)
  - model_name: claude
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
  
  # Local Ollama models (no API key needed)
  - model_name: local-llama
    litellm_params:
      model: ollama/llama2
      api_base: http://localhost:11434

# Optional: Enable caching
# cache:
#   type: redis
#   host: localhost
#   port: 6379

# Optional: Set budget limits
# litellm_settings:
#   max_budget: 100.0
#   budget_duration: 30d
EOFCONFIG
    
    print_success "Created example configuration: $CONFIG_FILE"
    print_warning "Edit this file to add your API keys and models"
    print_status ""
fi

# Validate configuration
print_status "Validating configuration..."
if ! litellm --config "$CONFIG_FILE" --test 2>/dev/null; then
    print_warning "Configuration validation had warnings, but continuing..."
fi

# Start the proxy
print_status ""
print_success "ğŸš€ Starting LiteLLM Multi-Model Proxy"
print_status "Configuration: $CONFIG_FILE"
print_status "Host: $HOST"
print_status "Port: $PORT"
print_status ""
print_success "API Endpoints:"
print_success "  - Chat: http://$HOST:$PORT/v1/chat/completions"
print_success "  - Embeddings: http://$HOST:$PORT/v1/embeddings"
print_success "  - Models: http://$HOST:$PORT/v1/models"
print_success "  - Health: http://$HOST:$PORT/health"
print_status ""
print_status "OpenAI-compatible! Works with any OpenAI SDK or tool."
print_status ""
print_status "Press Ctrl+C to stop"
print_status "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
print_status ""

# Run LiteLLM proxy
exec litellm --config "$CONFIG_FILE" --port "$PORT" --host "$HOST"

